{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#! Python3\n",
    "# -*- coding: utf-8 -*-\n",
    "# 異分野共著　process I : Make ID list\n",
    "\n",
    "\n",
    "import csv, os, glob\n",
    "import pandas  as pd\n",
    "import numpy as np\n",
    "\n",
    "#0.事前準備\n",
    "#複数ファイルがある場合: fromSciValというフォルダを作り、SciValからDLしたファイルをそのまま入れておく\n",
    "#DLした項目によって、下記を変更\n",
    "col_names =  ['Title', 'Authors', 'Number of Authors', 'Scopus Author Ids', 'Year', 'Scopus Source title', 'SNIP 2017', 'Citations',\n",
    "       'Field-Weighted Outputs in Top Citation Percentiles, per percentile', 'DOI', 'Publication-type', 'EID',\n",
    "       'All Science Journal Classification (ASJC)']\n",
    "\n",
    "#fromSciVal内にあるファイルの名を変更、一つのリストにまとめる\n",
    "publicationsAll = pd.DataFrame(index=[], columns=col_names)\n",
    "# カレントディレクトリの全ファイルをループする\n",
    "for csv_file in os.listdir('fromSciVal/.'):\n",
    "    if not csv_file.endswith('.csv'):\n",
    "        continue # CSVファイルでなければスキップ\n",
    "\n",
    "    print('Change name: ' + csv_file + '...')\n",
    "    df = pd.read_csv('fromSciVal/' + csv_file, header = None, dtype ='object',names = col_names)\n",
    "    dfname = str(df.iat[0,1] + \"_\" +df.iat[1,1])\n",
    "    os.rename('fromSciVal/' + csv_file, 'fromSciVal/' + dfname + '_190207.csv')\n",
    "    df2 = df.dropna(axis = 0, subset=['All Science Journal Classification (ASJC)'])\n",
    "    df3= df2[df2['Title'] != 'Title']\n",
    "    publicationsAll = pd.concat([publicationsAll, df3], ignore_index=True)\n",
    "\n",
    "publicationsAll.to_csv('Publications_KU2015-17.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all publications:104799\n",
      "Target publications (単著以外):99327\n"
     ]
    }
   ],
   "source": [
    "# 1. publicationsAllより、重複削除、単著論文を除外、HAPは著者情報が500までしかDLされないが、とりあえずそのまま（APIで確認し直す）\n",
    "publicationsAll['Number of Authors'] = publicationsAll['Number of Authors'].astype(int)\n",
    "uniquepubdf = publicationsAll.drop_duplicates()\n",
    "targetdf = uniquepubdf[uniquepubdf['Number of Authors'] > 1]\n",
    "print('all publications:' + str(len(uniquepubdf)))\n",
    "print('Target publications (単著以外):' + str(len(targetdf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'Authors', 'Number of Authors', 'Scopus Author Ids', 'Year',\n",
       "       'Scopus Source title', 'SNIP 2017', 'Citations',\n",
       "       'Field-Weighted Outputs in Top Citation Percentiles, per percentile',\n",
       "       'DOI', 'Publication-type', 'EID',\n",
       "       'All Science Journal Classification (ASJC)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#2. Author ID を抽出(重複あり)、数を確認\n",
    "aIds = []\n",
    "nofaIds = 0\n",
    "for i, row in targetdf.iterrows():\n",
    "    author = [row['Scopus Author Ids']]\n",
    "    author = author[0].split(', ')\n",
    "    aIds.extend(author)\n",
    "    if row['Number of Authors'] > 500:\n",
    "        nofa = 500\n",
    "    else:\n",
    "        nofa = row['Number of Authors']\n",
    "    nofaIds = nofaIds + nofa\n",
    "\n",
    "    #HAP以外でIDの数とAuthor数が一致しない場合、そのscopusIDを出力（一応確認のため）\n",
    "    if len(author) != nofa:\n",
    "        print(row['EID'])\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "aIdsdf = pd.DataFrame(aIds, columns=['Author ID'])\n",
    "\n",
    "#Authorの数とIDの数が一致するか確認\n",
    "print('Author Ids count: ' + str(len(aIdsdf)))\n",
    "print('Number of Authors: ' + str(nofaIds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Author IDs: 279553\n"
     ]
    }
   ],
   "source": [
    "#3. Unique Author ID listを作る\n",
    "uniqueaIds = aIdsdf.drop_duplicates()\n",
    "print('Unique Author IDs: ' + str(len(uniqueaIds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueaIds.to_csv('UniqueAuthorIdsList.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
